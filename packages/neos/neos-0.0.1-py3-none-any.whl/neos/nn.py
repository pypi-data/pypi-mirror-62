# AUTOGENERATED! DO NOT EDIT! File to edit: 00_nn.ipynb (unless otherwise specified).

__all__ = ['one_hot', 'simple_classifier', 'three_blob_classifier', 'two_blob_classifier']

# Cell
import jax
import jax.numpy as np
from jax.experimental import stax
from jax.experimental.stax import Dense, Relu, LogSoftmax
from jax import jit, grad, random
from jax.random import multivariate_normal, PRNGKey
from jax.config import config
from jax.experimental import optimizers

# avoid those precision errors!
config.update("jax_enable_x64", True)

import numpy.random as npr
import itertools
import time
import matplotlib.pyplot as plt

# Cell
def one_hot(x, k, dtype=np.float32):
    """Create a one-hot encoding of x of size k. Taken from jax/examples."""
    return np.array(x[:, None] == np.arange(k), dtype)

# Cell
class simple_classifier:
    """A simple classifier trained on separating two close normal distributions. Modelled after jax/examples/mnist_classifier.py."""

    def __init__(self, rng=PRNGKey(0), dist=1, nodes=10):

        # Set up architecture
        self.init_random_params, self.predict = stax.serial(
            Dense(nodes), Relu, Dense(2), LogSoftmax
        )

        # save random seed and dist
        self.jrng = rng
        self.dist = dist

        # sample from two 2D gaussians that are dist apart in one dim
        sig = multivariate_normal(
            PRNGKey(1),
            np.asarray([2, 5]),
            np.asarray([[1, 0], [0, 1]]),
            shape=(1, 10000),
        )[0]
        bkg = multivariate_normal(
            PRNGKey(1),
            np.asarray([2 + dist, 6]),
            np.asarray([[1, 0], [0, 1]]),
            shape=(1, 10000),
        )[0]
        # 1 = signal, 0 = background
        sig_lables = np.ones((1, 10000))[0]
        bkg_lables = np.zeros((1, 10000))[0]

        # create training data
        self.train_data = np.concatenate((sig, bkg))
        self.train_labels = one_hot(np.concatenate((sig_lables, bkg_lables)), 2)

        # placeholder for neural network parameters
        self.params = []

    def __call__(self, x, params=None):
        if params == None:
            params = self.params
        return self.predict(params, x)

    def plot_train_data(self):
        plt.scatter(
            self.train_data[:, 0],
            self.train_data[:, 1],
            c=self.train_labels[:, 0],
            alpha=0.2,
            s=1.4,
        )
        plt.show()

    def train(self, step_size=0.001, num_epochs=3, batch_size=16, momentum_mass=0.9):
        """Train the classifier using the specified hyperparameters."""
        # set up batch number based on num of examples and batch_size
        num_train = self.train_data.shape[0]
        num_complete_batches, leftover = divmod(num_train, batch_size)
        num_batches = num_complete_batches + bool(leftover)

        # batching helper function
        def data_stream():
            rng = npr.RandomState(0)
            while True:
                perm = rng.permutation(num_train)
                for i in range(num_batches):
                    batch_idx = perm[i * batch_size : (i + 1) * batch_size]
                    yield self.train_data[batch_idx], self.train_labels[batch_idx]

        # can call next(batches) to get a batch
        batches = data_stream()

        # set up momentum optimiser
        opt_init, opt_update, get_params = optimizers.momentum(
            step_size, mass=momentum_mass
        )

        # metrics:
        def loss(params, batch):
            inputs, targets = batch
            preds = self.predict(params, inputs)
            return -np.mean(np.sum(preds * targets, axis=1))

        def accuracy(params, batch):
            inputs, targets = batch
            target_class = np.argmax(targets, axis=1)
            predicted_class = np.argmax(self.predict(params, inputs), axis=1)
            return np.mean(predicted_class == target_class)

        # parameter update function
        @jit
        def update(i, opt_state, batch):
            params = get_params(opt_state)
            return opt_update(i, grad(loss)(params, batch), opt_state)

        _, init_params = self.init_random_params(self.jrng, (-1, 2))
        if self.params == []:
            opt_state = opt_init(init_params)
        else:
            opt_state = opt_init(self.params)
        itercount = itertools.count()

        from celluloid import Camera

        fig = plt.figure()
        camera = Camera(fig)

        # train loop
        print("\nStarting training...")
        for epoch in range(num_epochs):
            start_time = time.time()
            for _ in range(num_batches):
                opt_state = update(next(itercount), opt_state, next(batches))
                # print("Loss {}".format(loss(params,next(batches))))
            epoch_time = time.time() - start_time

            params = get_params(opt_state)
            self.params = params
            self.plot_decision(True)
            camera.snap()
            train_acc = accuracy(params, (self.train_data, self.train_labels))
            # test_acc = accuracy(self.params, (test_data, test_labels))
            print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
            print("Training set accuracy {}".format(train_acc))
            # print("Test set accuracy {}".format(test_acc))

        animation = camera.animate()

        # keep trained parameters
        self.params = params
        return animation

    def plot_decision(self, plot_data=False, cmap="plasma", levels=50):
        """Helper function to plot decision contours of classifier."""
        delta = 0.05
        x = np.arange(-2.0, 11.0, delta)
        y = np.arange(0, 11.0, delta)
        X, Y = np.meshgrid(x, y)
        pairs = np.dstack([X, Y]).reshape(-1, 2)
        Z = np.exp(self.predict(self.params, pairs))[:, 1].reshape(X.shape)
        # fig, ax = plt.subplots()
        plt.contourf(X, Y, Z, levels=levels, cmap="binary")  # cmap)
        # ax.set_title('Decision contours of simple_classifier')
        if plot_data:
            plt.scatter(
                self.train_data[:, 0],
                self.train_data[:, 1],
                c=self.train_labels[:, 0],
                alpha=0.2,
                s=1.2,
            )
        CS = plt.contour(X, Y, Z, cmap=cmap)
        plt.clabel(CS, inline=1, fontsize=10)
        plt.axis("off")
        # plt.savefig("test.png", bbox_inches='tight',dpi=324)
        # plt.show()

    def gen_sig(self, nev, rng=PRNGKey(21)):
        return multivariate_normal(
            rng, np.asarray([2, 5]), np.asarray([[1, 0], [0, 1]]), shape=(1, nev)
        )[0]

    def gen_bkg(self, nev, rng=PRNGKey(22)):
        return multivariate_normal(
            rng,
            np.asarray([2 + self.dist, 5]),
            np.asarray([[1, 0], [0, 1]]),
            shape=(1, nev),
        )[0]

# Cell
class three_blob_classifier:
    """A simple classifier trained on separating two close normal distributions, but one has two variations. Modelled after jax/examples/mnist_classifier.py."""

    def __init__(self, keys=[1, 2, 3], nodes=5):

        # Set up architecture
        self.init_random_params, self.predict = stax.serial(
            Dense(nodes), Relu, Dense(2), LogSoftmax
        )

        k1, k2, k3 = keys
        sig = jax.random.multivariate_normal(
            jax.random.PRNGKey(k1),
            jax.numpy.asarray([2, 5]),
            jax.numpy.asarray([[1, 0.0], [0.0, 1]]),
            shape=(1, 10000),
        )[0]
        bkg1 = jax.random.multivariate_normal(
            jax.random.PRNGKey(k2),
            jax.numpy.asarray([4.5, 7.5]),
            jax.numpy.asarray([[1, 0.6], [0.6, 1]]),
            shape=(1, 5000),
        )[0]
        bkg2 = jax.random.multivariate_normal(
            jax.random.PRNGKey(k3),
            jax.numpy.asarray([4.5, 3.5]),
            jax.numpy.asarray([[1.7, 0.2], [0.2, 1]]),
            shape=(1, 5000),
        )[0]

        # 1 = signal, 0 = background
        sig_lables = np.ones((1, 10000))[0]
        bkg_lables = np.zeros((1, 10000))[0]

        # create training data
        self.train_data = np.concatenate((sig, bkg1, bkg2))
        self.train_labels = one_hot(np.concatenate((sig_lables, bkg_lables)), 2)

        # placeholder for neural network parameters
        self.params = []

    def __call__(self, x, params=None):
        if params == None:
            params = self.params
        return self.predict(params, x)

    def plot_train_data(self):
        plt.scatter(
            self.train_data[:, 0],
            self.train_data[:, 1],
            c=self.train_labels[:, 0],
            alpha=0.2,
            s=1.4,
        )
        #plt.show()

    def train(self, step_size=0.001, num_epochs=3, batch_size=16, momentum_mass=0.9):
        """Train the classifier using the specified hyperparameters."""
        # set up batch number based on num of examples and batch_size
        num_train = self.train_data.shape[0]
        num_complete_batches, leftover = divmod(num_train, batch_size)
        num_batches = num_complete_batches + bool(leftover)

        # batching helper function
        def data_stream():
            rng = npr.RandomState(0)
            while True:
                perm = rng.permutation(num_train)
                for i in range(num_batches):
                    batch_idx = perm[i * batch_size : (i + 1) * batch_size]
                    yield self.train_data[batch_idx], self.train_labels[batch_idx]

        # can call next(batches) to get a batch
        batches = data_stream()

        # set up momentum optimiser
        opt_init, opt_update, get_params = optimizers.momentum(
            step_size, mass=momentum_mass
        )

        # metrics:
        def loss(params, batch):
            inputs, targets = batch
            preds = self.predict(params, inputs)
            return -np.mean(np.sum(preds * targets, axis=1))

        def accuracy(params, batch):
            inputs, targets = batch
            target_class = np.argmax(targets, axis=1)
            predicted_class = np.argmax(self.predict(params, inputs), axis=1)
            return np.mean(predicted_class == target_class)

        # parameter update function
        @jit
        def update(i, opt_state, batch):
            params = get_params(opt_state)
            return opt_update(i, grad(loss)(params, batch), opt_state)

        _, init_params = self.init_random_params(jax.random.PRNGKey(12), (-1, 2))
        if self.params == []:
            opt_state = opt_init(init_params)
        else:
            opt_state = opt_init(self.params)
        itercount = itertools.count()

        from celluloid import Camera

        fig = plt.figure()
        camera = Camera(fig)

        # train loop
        print("\nStarting training...")
        for epoch in range(num_epochs):
            start_time = time.time()
            for _ in range(num_batches):
                opt_state = update(next(itercount), opt_state, next(batches))
                # print("Loss {}".format(loss(params,next(batches))))
            epoch_time = time.time() - start_time

            params = get_params(opt_state)
            self.params = params
            self.plot_decision(plot_data=True, cbar=True)
            camera.snap()
            # if epoch!=num_epochs-1: plt.clf()
            train_acc = accuracy(params, (self.train_data, self.train_labels))
            # test_acc = accuracy(self.params, (test_data, test_labels))
            print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
            print("Training set accuracy {}".format(train_acc))
            # print("Test set accuracy {}".format(test_acc))

        animation = camera.animate()

        # keep trained parameters
        self.params = params
        return animation

    def plot_decision(self, plot_data=False, cmap="plasma", levels=50, cbar=False):
        """Helper function to plot decision contours of classifier."""
        delta = 0.05
        x = np.arange(-2.0, 11.0, delta)
        y = np.arange(0, 11.0, delta)
        X, Y = np.meshgrid(x, y)
        pairs = np.dstack([X, Y]).reshape(-1, 2)
        Z = np.exp(self.predict(self.params, pairs))[:, 1].reshape(X.shape)
        # fig, ax = plt.subplots()
        plo = plt.contourf(X, Y, Z, levels=levels, cmap="binary")
        # if cbar: plt.colorbar(plo)
        # ax.set_title('Decision contours of simple_classifier')
        if plot_data:
            plt.scatter(
                self.train_data[:, 0],
                self.train_data[:, 1],
                c=self.train_labels[:, 0],
                alpha=0.2,
                s=1.2,
            )
        CS = plt.contour(X, Y, Z, cmap=cmap)
        plt.clabel(CS, inline=1, fontsize=10)
        plt.axis("off")
        # plt.savefig("test.png", bbox_inches='tight',dpi=324)
        # plt.show()

# Cell
class two_blob_classifier:
    """A simple classifier trained on separating two close normal distributions, but one has two variations. Modelled after jax/examples/mnist_classifier.py."""

    def __init__(self, keys=[1, 2, 3], nodes=5):

        # Set up architecture
        self.init_random_params, self.predict = stax.serial(
            Dense(nodes), Relu, Dense(2), LogSoftmax
        )

        k1, k2, k3 = keys
        sig = jax.random.multivariate_normal(
            jax.random.PRNGKey(k1),
            jax.numpy.asarray([2, 5]),
            jax.numpy.asarray([[1, 0.0], [0.0, 1]]),
            shape=(1, 5000),
        )[0]
        bkg1 = jax.random.multivariate_normal(
            jax.random.PRNGKey(k2),
            jax.numpy.asarray([4, 6]),
            jax.numpy.asarray([[1, 0.6], [0.6, 1]]),
            shape=(1, 5000),
        )[0]


        sig_lables = np.ones((1, 5000))[0]
        bkg_lables = np.zeros((1, 5000))[0]

        self.train_data = np.concatenate((sig, bkg1))
        self.train_labels = one_hot(np.concatenate((sig_lables, bkg_lables)), 2)

        # placeholder for neural network parameters
        self.params = []

    def __call__(self, x, params=None):
        if params == None:
            params = self.params
        return self.predict(params, x)

    def plot_train_data(self):
        plt.scatter(
            self.train_data[:, 0],
            self.train_data[:, 1],
            c=self.train_labels[:, 0],
            alpha=0.2,
            s=1.4,
        )
        plt.show()

    def train(self, step_size=0.001, num_epochs=3, batch_size=16, momentum_mass=0.9):
        """Train the classifier using the specified hyperparameters."""
        # set up batch number based on num of examples and batch_size
        num_train = self.train_data.shape[0]
        num_complete_batches, leftover = divmod(num_train, batch_size)
        num_batches = num_complete_batches + bool(leftover)

        # batching helper function
        def data_stream():
            rng = npr.RandomState(0)
            while True:
                perm = rng.permutation(num_train)
                for i in range(num_batches):
                    batch_idx = perm[i * batch_size : (i + 1) * batch_size]
                    yield self.train_data[batch_idx], self.train_labels[batch_idx]

        # can call next(batches) to get a batch
        batches = data_stream()

        # set up momentum optimiser
        opt_init, opt_update, get_params = optimizers.momentum(
            step_size, mass=momentum_mass
        )

        # metrics:
        def loss(params, batch):
            inputs, targets = batch
            preds = self.predict(params, inputs)
            return -np.mean(np.sum(preds * targets, axis=1))

        def accuracy(params, batch):
            inputs, targets = batch
            target_class = np.argmax(targets, axis=1)
            predicted_class = np.argmax(self.predict(params, inputs), axis=1)
            return np.mean(predicted_class == target_class)

        # parameter update function
        @jit
        def update(i, opt_state, batch):
            params = get_params(opt_state)
            return opt_update(i, grad(loss)(params, batch), opt_state)

        _, init_params = self.init_random_params(jax.random.PRNGKey(12), (-1, 2))
        if self.params == []:
            opt_state = opt_init(init_params)
        else:
            opt_state = opt_init(self.params)
        itercount = itertools.count()

        from celluloid import Camera

        fig = plt.figure()
        camera = Camera(fig)

        # train loop
        print("\nStarting training...")
        for epoch in range(num_epochs):
            start_time = time.time()
            for _ in range(num_batches):
                opt_state = update(next(itercount), opt_state, next(batches))
                # print("Loss {}".format(loss(params,next(batches))))
            epoch_time = time.time() - start_time

            params = get_params(opt_state)
            self.params = params
            self.plot_decision(plot_data=True, cbar=True)
            camera.snap()
            # if epoch!=num_epochs-1: plt.clf()
            train_acc = accuracy(params, (self.train_data, self.train_labels))
            # test_acc = accuracy(self.params, (test_data, test_labels))
            print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
            print("Training set accuracy {}".format(train_acc))
            # print("Test set accuracy {}".format(test_acc))

        animation = camera.animate()

        # keep trained parameters
        self.params = params
        return animation

    def plot_decision(self, plot_data=False, cmap="plasma", levels=50, cbar=False):
        """Helper function to plot decision contours of classifier."""
        delta = 0.05
        x = np.arange(-2.0, 11.0, delta)
        y = np.arange(0, 11.0, delta)
        X, Y = np.meshgrid(x, y)
        pairs = np.dstack([X, Y]).reshape(-1, 2)
        Z = np.exp(self.predict(self.params, pairs))[:, 1].reshape(X.shape)
        # fig, ax = plt.subplots()
        plo = plt.contourf(X, Y, Z, levels=levels, cmap="binary")
        # if cbar: plt.colorbar(plo)
        # ax.set_title('Decision contours of simple_classifier')
        if plot_data:
            plt.scatter(
                self.train_data[:, 0],
                self.train_data[:, 1],
                c=self.train_labels[:, 0],
                alpha=0.2,
                s=1.2,
            )
        CS = plt.contour(X, Y, Z, cmap=cmap)
        plt.clabel(CS, inline=1, fontsize=10)
        plt.axis("off")
        # plt.savefig("test.png", bbox_inches='tight',dpi=324)
        # plt.show()